"""Exploitability analysis module (Hongju-style).

Implements:
- Game-theoretic exploitability computation
- Best response calculation
- Empirical outcome classification
- Payoff analysis

Key concepts:
- Exploitability: How much can a best-responding opponent gain?
- Best Response: Optimal strategy against a fixed opponent policy
- Outcome Classification: Categorize interaction patterns
"""

import numpy as np
from typing import Dict, Tuple, Optional, List, Union
from dataclasses import dataclass
from enum import Enum


# Standard Prisoner's Dilemma payoff matrix
# Row = my action, Col = opponent action
# Values are (my_payoff, opponent_payoff)
COOPERATE = 0
DEFECT = 1

# Payoff matrix for row player (agent i)
# PD_PAYOFF_I[my_action, their_action] = my_payoff
PD_PAYOFF_I = np.array([
    [3, 0],  # I cooperate: CC=3, CD=0
    [5, 1],  # I defect: DC=5, DD=1
])

# Payoff matrix for column player (agent j)
# PD_PAYOFF_J[i_action, j_action] = j's payoff
PD_PAYOFF_J = np.array([
    [3, 5],  # i cooperates: CC=3, CD=5 (j gets temptation)
    [0, 1],  # i defects: DC=0 (j gets sucker), DD=1
])


class OutcomeType(Enum):
    """Classification of interaction outcomes."""
    MUTUAL_COOPERATION = "mutual_cooperation"
    MUTUAL_DEFECTION = "mutual_defection"
    I_EXPLOITED = "i_exploited"  # i cooperates, j defects, j gains
    J_EXPLOITED = "j_exploited"  # j cooperates, i defects, i gains
    CYCLING = "cycling"  # Alternating or mixed patterns
    UNKNOWN = "unknown"


@dataclass
class OutcomeFrequencies:
    """Frequencies of joint outcomes."""
    CC: float  # Mutual cooperation
    CD: float  # i cooperates, j defects
    DC: float  # i defects, j cooperates
    DD: float  # Mutual defection

    def to_dict(self) -> Dict[str, float]:
        return {"CC": self.CC, "CD": self.CD, "DC": self.DC, "DD": self.DD}

    @classmethod
    def from_actions(cls, actions_i: np.ndarray, actions_j: np.ndarray) -> "OutcomeFrequencies":
        """Compute outcome frequencies from action sequences."""
        n = len(actions_i)
        if n == 0:
            return cls(CC=0.0, CD=0.0, DC=0.0, DD=0.0)

        cc = np.sum((actions_i == COOPERATE) & (actions_j == COOPERATE)) / n
        cd = np.sum((actions_i == COOPERATE) & (actions_j == DEFECT)) / n
        dc = np.sum((actions_i == DEFECT) & (actions_j == COOPERATE)) / n
        dd = np.sum((actions_i == DEFECT) & (actions_j == DEFECT)) / n

        return cls(CC=cc, CD=cd, DC=dc, DD=dd)


@dataclass
class TrajectoryStats:
    """Statistics from a simulation trajectory."""
    # Action frequencies
    cooperation_rate_i: float
    cooperation_rate_j: float

    # Outcome frequencies
    outcome_freq: OutcomeFrequencies

    # Payoffs
    payoff_i_mean: float
    payoff_i_std: float
    payoff_j_mean: float
    payoff_j_std: float
    payoff_gap: float  # i's payoff - j's payoff

    # Switching rates (how often agents change actions)
    switching_rate_i: float
    switching_rate_j: float

    # Outcome classification
    outcome_type: OutcomeType

    # Exploitability scores
    exploitability_i: float
    exploitability_j: float


def compute_best_response(
    opponent_policy: np.ndarray,
    payoff_matrix: np.ndarray = PD_PAYOFF_I,
) -> np.ndarray:
    """
    Compute the best response to an opponent's mixed strategy.

    In a 2x2 game, the best response is the action that maximizes
    expected payoff given the opponent's policy.

    Args:
        opponent_policy: [P(C), P(D)] - opponent's action probabilities
        payoff_matrix: Payoff matrix where payoff_matrix[my_action, their_action] = my_payoff

    Returns:
        Best response policy [P(C), P(D)]
    """
    # Expected payoff for each of my actions
    expected_payoffs = payoff_matrix @ opponent_policy

    # Best response is the action with highest expected payoff
    best_action = np.argmax(expected_payoffs)

    # Return pure strategy best response
    br = np.zeros(2)
    br[best_action] = 1.0

    return br


def compute_expected_payoff(
    policy_i: np.ndarray,
    policy_j: np.ndarray,
    payoff_matrix: np.ndarray = PD_PAYOFF_I,
) -> float:
    """
    Compute expected payoff for player i given both policies.

    E[payoff_i] = sum_{a_i, a_j} P(a_i) * P(a_j) * payoff(a_i, a_j)

    Args:
        policy_i: [P(C), P(D)] for player i
        policy_j: [P(C), P(D)] for player j
        payoff_matrix: Payoff matrix for player i

    Returns:
        Expected payoff for player i
    """
    return policy_i @ payoff_matrix @ policy_j


def compute_exploitability(
    policy: np.ndarray,
    payoff_matrix_opponent: np.ndarray = PD_PAYOFF_J,
    payoff_matrix_self: np.ndarray = PD_PAYOFF_I,
) -> float:
    """
    Compute how exploitable a policy is.

    Exploitability measures how much an optimal opponent can gain
    compared to the Nash equilibrium value.

    Exploitability(π_i) = U_j(BR(π_i), π_i) - V_j(Nash)

    Where V_j(Nash) is the Nash equilibrium value for player j (which is 1 in PD).

    A policy that makes an opponent better off than Nash is exploitable.
    In PD:
    - Always defect (Nash): exploitability = 0 (opponent gets 1)
    - Always cooperate: exploitability = 4 (opponent gets 5 vs Nash 1)

    Args:
        policy: The policy to evaluate [P(C), P(D)]
        payoff_matrix_opponent: Opponent's payoff matrix
        payoff_matrix_self: Self payoff matrix (for computing BR)

    Returns:
        Exploitability score (higher = more exploitable)
    """
    # Best response to this policy (from opponent's perspective)
    # We need opponent's payoff for each of their actions given our policy
    # payoff_matrix_opponent[our_action, their_action] = their_payoff
    # Expected payoff for j's action a_j: sum_a_i policy[a_i] * payoff_j[a_i, a_j]
    # This equals policy @ payoff_matrix_opponent (result is [payoff_if_j_C, payoff_if_j_D])
    opponent_expected_payoffs = policy @ payoff_matrix_opponent

    # Opponent's best response is the action maximizing their expected payoff
    br_action = np.argmax(opponent_expected_payoffs)

    # Opponent's payoff under best response
    u_br = opponent_expected_payoffs[br_action]

    # Nash equilibrium value for opponent in PD is 1 (mutual defection)
    nash_value = 1.0

    return u_br - nash_value


def compute_switching_rate(actions: np.ndarray) -> float:
    """
    Compute how often an agent switches actions.

    Args:
        actions: Sequence of actions (0 or 1)

    Returns:
        Fraction of timesteps where action changed
    """
    if len(actions) <= 1:
        return 0.0

    switches = np.sum(np.diff(actions) != 0)
    return switches / (len(actions) - 1)


def classify_outcome(
    actions_i: np.ndarray,
    actions_j: np.ndarray,
    payoffs_i: Optional[np.ndarray] = None,
    payoffs_j: Optional[np.ndarray] = None,
    cooperation_threshold: float = 0.7,
    defection_threshold: float = 0.7,
    exploitation_coop_threshold: float = 0.6,
    exploitation_def_threshold: float = 0.4,
    payoff_gap_threshold: float = 1.0,
) -> OutcomeType:
    """
    Classify the outcome of an interaction trajectory.

    Args:
        actions_i: Agent i's action sequence
        actions_j: Agent j's action sequence
        payoffs_i: Agent i's payoff sequence (optional)
        payoffs_j: Agent j's payoff sequence (optional)
        cooperation_threshold: Threshold for mutual cooperation
        defection_threshold: Threshold for mutual defection
        exploitation_coop_threshold: Cooperation rate threshold for exploitation
        exploitation_def_threshold: Defection rate threshold for exploitation
        payoff_gap_threshold: Minimum payoff gap for exploitation

    Returns:
        OutcomeType classification
    """
    if len(actions_i) == 0 or len(actions_j) == 0:
        return OutcomeType.UNKNOWN

    # Compute outcome frequencies
    freq = OutcomeFrequencies.from_actions(actions_i, actions_j)

    # Cooperation rates
    coop_rate_i = np.mean(actions_i == COOPERATE)
    coop_rate_j = np.mean(actions_j == COOPERATE)

    # Compute payoffs if not provided
    if payoffs_i is None or payoffs_j is None:
        payoffs_i, payoffs_j = compute_payoffs_from_actions(actions_i, actions_j)

    payoff_gap = np.mean(payoffs_i) - np.mean(payoffs_j)

    # Classification logic
    if freq.CC >= cooperation_threshold:
        return OutcomeType.MUTUAL_COOPERATION

    elif freq.DD >= defection_threshold:
        return OutcomeType.MUTUAL_DEFECTION

    elif (coop_rate_i >= exploitation_coop_threshold and
          coop_rate_j <= exploitation_def_threshold and
          payoff_gap < -payoff_gap_threshold):
        return OutcomeType.I_EXPLOITED

    elif (coop_rate_j >= exploitation_coop_threshold and
          coop_rate_i <= exploitation_def_threshold and
          payoff_gap > payoff_gap_threshold):
        return OutcomeType.J_EXPLOITED

    else:
        return OutcomeType.CYCLING


def compute_payoffs_from_actions(
    actions_i: np.ndarray,
    actions_j: np.ndarray,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Compute payoff sequences from action sequences.

    Args:
        actions_i: Agent i's actions
        actions_j: Agent j's actions

    Returns:
        (payoffs_i, payoffs_j) - Payoff sequences
    """
    n = len(actions_i)
    payoffs_i = np.zeros(n)
    payoffs_j = np.zeros(n)

    for t in range(n):
        a_i, a_j = int(actions_i[t]), int(actions_j[t])
        payoffs_i[t] = PD_PAYOFF_I[a_i, a_j]
        payoffs_j[t] = PD_PAYOFF_J[a_i, a_j]

    return payoffs_i, payoffs_j


class PayoffAnalysis:
    """Utilities for analyzing payoffs in PD interactions."""

    @staticmethod
    def compute_cumulative_payoffs(
        actions_i: np.ndarray,
        actions_j: np.ndarray,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Compute cumulative payoff sequences."""
        payoffs_i, payoffs_j = compute_payoffs_from_actions(actions_i, actions_j)
        return np.cumsum(payoffs_i), np.cumsum(payoffs_j)

    @staticmethod
    def compute_average_payoffs(
        actions_i: np.ndarray,
        actions_j: np.ndarray,
    ) -> Tuple[float, float]:
        """Compute average payoffs."""
        payoffs_i, payoffs_j = compute_payoffs_from_actions(actions_i, actions_j)
        return np.mean(payoffs_i), np.mean(payoffs_j)

    @staticmethod
    def compute_payoff_stats(
        actions_i: np.ndarray,
        actions_j: np.ndarray,
    ) -> Dict[str, float]:
        """Compute comprehensive payoff statistics."""
        payoffs_i, payoffs_j = compute_payoffs_from_actions(actions_i, actions_j)

        return {
            "mean_i": np.mean(payoffs_i),
            "mean_j": np.mean(payoffs_j),
            "std_i": np.std(payoffs_i),
            "std_j": np.std(payoffs_j),
            "total_i": np.sum(payoffs_i),
            "total_j": np.sum(payoffs_j),
            "gap": np.mean(payoffs_i) - np.mean(payoffs_j),
            "efficiency": (np.mean(payoffs_i) + np.mean(payoffs_j)) / 6.0,  # Max is 6 (CC)
        }

    @staticmethod
    def pareto_efficiency(
        actions_i: np.ndarray,
        actions_j: np.ndarray,
    ) -> float:
        """
        Compute Pareto efficiency of outcome.

        Pareto efficiency = (sum of payoffs) / (max possible sum)
        Max possible is 6 (mutual cooperation: 3+3)
        Min possible is 2 (mutual defection: 1+1)

        Returns value in [0, 1] where 1 is Pareto optimal (mutual cooperation)
        """
        payoffs_i, payoffs_j = compute_payoffs_from_actions(actions_i, actions_j)
        total = np.mean(payoffs_i) + np.mean(payoffs_j)

        # Normalize to [0, 1]
        # Min = 2 (DD), Max = 6 (CC)
        return (total - 2) / 4


class OutcomeClassifier:
    """Classifier for interaction outcomes with configurable thresholds."""

    def __init__(
        self,
        cooperation_threshold: float = 0.7,
        defection_threshold: float = 0.7,
        exploitation_coop_threshold: float = 0.6,
        exploitation_def_threshold: float = 0.4,
        payoff_gap_threshold: float = 1.0,
    ):
        self.cooperation_threshold = cooperation_threshold
        self.defection_threshold = defection_threshold
        self.exploitation_coop_threshold = exploitation_coop_threshold
        self.exploitation_def_threshold = exploitation_def_threshold
        self.payoff_gap_threshold = payoff_gap_threshold

    def classify(
        self,
        actions_i: np.ndarray,
        actions_j: np.ndarray,
        payoffs_i: Optional[np.ndarray] = None,
        payoffs_j: Optional[np.ndarray] = None,
    ) -> OutcomeType:
        """Classify outcome using instance thresholds."""
        return classify_outcome(
            actions_i=actions_i,
            actions_j=actions_j,
            payoffs_i=payoffs_i,
            payoffs_j=payoffs_j,
            cooperation_threshold=self.cooperation_threshold,
            defection_threshold=self.defection_threshold,
            exploitation_coop_threshold=self.exploitation_coop_threshold,
            exploitation_def_threshold=self.exploitation_def_threshold,
            payoff_gap_threshold=self.payoff_gap_threshold,
        )

    def classify_with_details(
        self,
        actions_i: np.ndarray,
        actions_j: np.ndarray,
    ) -> Dict[str, Union[OutcomeType, float, Dict]]:
        """Classify with full statistics."""
        payoffs_i, payoffs_j = compute_payoffs_from_actions(actions_i, actions_j)
        freq = OutcomeFrequencies.from_actions(actions_i, actions_j)

        outcome = self.classify(actions_i, actions_j, payoffs_i, payoffs_j)

        return {
            "outcome_type": outcome,
            "cooperation_rate_i": np.mean(actions_i == COOPERATE),
            "cooperation_rate_j": np.mean(actions_j == COOPERATE),
            "outcome_frequencies": freq.to_dict(),
            "payoff_i_mean": np.mean(payoffs_i),
            "payoff_j_mean": np.mean(payoffs_j),
            "payoff_gap": np.mean(payoffs_i) - np.mean(payoffs_j),
        }


class ExploitabilityAnalysis:
    """
    Complete exploitability analysis for PD interactions.

    Combines:
    - Game-theoretic exploitability
    - Empirical outcome classification
    - Payoff analysis
    """

    def __init__(self, outcome_classifier: Optional[OutcomeClassifier] = None):
        self.classifier = outcome_classifier or OutcomeClassifier()

    def analyze_trajectory(
        self,
        actions_i: np.ndarray,
        actions_j: np.ndarray,
    ) -> TrajectoryStats:
        """
        Perform complete analysis of a trajectory.

        Args:
            actions_i: Agent i's action sequence
            actions_j: Agent j's action sequence

        Returns:
            TrajectoryStats with all metrics
        """
        # Compute payoffs
        payoffs_i, payoffs_j = compute_payoffs_from_actions(actions_i, actions_j)

        # Outcome frequencies
        freq = OutcomeFrequencies.from_actions(actions_i, actions_j)

        # Cooperation rates
        coop_rate_i = np.mean(actions_i == COOPERATE)
        coop_rate_j = np.mean(actions_j == COOPERATE)

        # Switching rates
        switch_i = compute_switching_rate(actions_i)
        switch_j = compute_switching_rate(actions_j)

        # Outcome classification
        outcome = self.classifier.classify(actions_i, actions_j, payoffs_i, payoffs_j)

        # Exploitability (based on empirical policy)
        policy_i = np.array([coop_rate_i, 1 - coop_rate_i])
        policy_j = np.array([coop_rate_j, 1 - coop_rate_j])

        exploit_i = compute_exploitability(policy_i)
        exploit_j = compute_exploitability(policy_j)

        return TrajectoryStats(
            cooperation_rate_i=coop_rate_i,
            cooperation_rate_j=coop_rate_j,
            outcome_freq=freq,
            payoff_i_mean=np.mean(payoffs_i),
            payoff_i_std=np.std(payoffs_i),
            payoff_j_mean=np.mean(payoffs_j),
            payoff_j_std=np.std(payoffs_j),
            payoff_gap=np.mean(payoffs_i) - np.mean(payoffs_j),
            switching_rate_i=switch_i,
            switching_rate_j=switch_j,
            outcome_type=outcome,
            exploitability_i=exploit_i,
            exploitability_j=exploit_j,
        )

    def analyze_multiple_trajectories(
        self,
        trajectories: List[Tuple[np.ndarray, np.ndarray]],
    ) -> Dict[str, Union[float, Dict]]:
        """
        Analyze multiple trajectories and compute aggregate statistics.

        Args:
            trajectories: List of (actions_i, actions_j) tuples

        Returns:
            Aggregate statistics across all trajectories
        """
        stats_list = [
            self.analyze_trajectory(actions_i, actions_j)
            for actions_i, actions_j in trajectories
        ]

        # Aggregate statistics
        outcome_counts = {}
        for s in stats_list:
            outcome = s.outcome_type.value
            outcome_counts[outcome] = outcome_counts.get(outcome, 0) + 1

        n = len(stats_list)
        outcome_probs = {k: v / n for k, v in outcome_counts.items()}

        return {
            "n_trajectories": n,
            "outcome_distribution": outcome_probs,
            "mean_cooperation_i": np.mean([s.cooperation_rate_i for s in stats_list]),
            "mean_cooperation_j": np.mean([s.cooperation_rate_j for s in stats_list]),
            "mean_payoff_i": np.mean([s.payoff_i_mean for s in stats_list]),
            "mean_payoff_j": np.mean([s.payoff_j_mean for s in stats_list]),
            "mean_payoff_gap": np.mean([s.payoff_gap for s in stats_list]),
            "mean_exploitability_i": np.mean([s.exploitability_i for s in stats_list]),
            "mean_exploitability_j": np.mean([s.exploitability_j for s in stats_list]),
            "mean_switching_rate_i": np.mean([s.switching_rate_i for s in stats_list]),
            "mean_switching_rate_j": np.mean([s.switching_rate_j for s in stats_list]),
        }

    @staticmethod
    def compute_policy_exploitability(
        cooperation_rate: float,
    ) -> float:
        """
        Compute exploitability for a given cooperation rate.

        Args:
            cooperation_rate: Probability of cooperating

        Returns:
            Exploitability score
        """
        policy = np.array([cooperation_rate, 1 - cooperation_rate])
        return compute_exploitability(policy)

    @staticmethod
    def exploitability_curve(
        cooperation_rates: Optional[np.ndarray] = None,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Compute exploitability as a function of cooperation rate.

        Useful for visualization.

        Returns:
            (cooperation_rates, exploitability_scores)
        """
        if cooperation_rates is None:
            cooperation_rates = np.linspace(0, 1, 101)

        exploitability = np.array([
            ExploitabilityAnalysis.compute_policy_exploitability(c)
            for c in cooperation_rates
        ])

        return cooperation_rates, exploitability
