import logging
import numpy as np

from empathy.prisoners_dilemma.agent import EmpatheticAgent
from empathy.prisoners_dilemma.env import Environment

logging.basicConfig()
LOGGER = logging.getLogger()
LOGGER.setLevel(logging.INFO)


class Sim:
    def __init__(self, config: dict) -> None:
        
        # TODO: Validate config with pydantic
        self.config = config
        self.T = self.config["T"]
        self.K = self.config["K"]

        # Initialize agents
        self.agents = []
        for k in range(self.K):
            self.agents.append(EmpatheticAgent(config=self.config, agent_num=k))
        
        # Initialize environment and history dict
        self.env = Environment(K=self.K)
        self._initialize_history()

    def run(self, verbose: bool=True) -> dict:
        """ Run the action-perception loop for each K agent """
        actions = np.zeros(self.K)   # Used to store each agent's actions
        for t in range(self.T):   # Loop over simulation time steps
            o = self.env.step(actions=actions, t=t)   # Pass action environment and get observation
            for k in range(self.K):  # Loop over each agent in simulation
                
                # LOGGER.info(f"Agent index: {k}, time index: {t}")
                
                # Pass observation to each agent and get results from this step
                step_results = self.agents[k].step(t=t, o=o)   
                
                # Extract action for agent and store
                step_action = step_results['exp_action']   # The action for this step
                actions[k] = step_action                   # Running history of agent's actions for time step
                
                # Update history dict with results
                step_results["o"] = o                      # Store observation for time step in step results
                self._update_history(                      # Update history dict with step results
                    t=t, k=k, step_results=step_results)   
                
                # Log step results to console if verbose is selected
                if verbose:
                    self._log_step_results(t=t, k=k, step_results=step_results, step_action=step_action)
        
        # Store observations for t=0 (generated by agent instead of environment)
        self.history["results"]["o"][0] = np.array([self.agents[k].o_init[0] for k in range(self.K)])
        return self.history    
            
    def _log_step_results(self, t: int, k: int, step_results: dict, step_action: int) -> None:
        """ Prints the step results for each agent and time step. """
        
        LOGGER.info(f"t = {t}, Agent {k}")                               # Time step and agent
        LOGGER.info(f"qs = {step_results['qs']}")                        # Variational state posterior
        LOGGER.info(f"G = {step_results['exp_G']}")                      # Expected free energy
        LOGGER.info(f"q_pi = {step_results['exp_q_pi']}")                # Variational policy posterior
        LOGGER.info(f"p_u = {step_results['exp_p_u']}")                  # Action marginal
        action_name = self.config["actions"][int(step_action)]
        LOGGER.info(f"chosen action: {action_name}")     # Chosen action (string mapping)
        LOGGER.info("================")
    
    def _initialize_history(self) -> None:
        """ Initializes the dict used to store the simulation history """
        
        # Prepare indices needed to construct history vector
        num_state_categories = self.config["A"][0][0].shape[1]
        num_policies = len(self.agents[0].policies)
        num_actions = len(self.config["actions"])
        
        # Construct empty history dict with known dimensions
        self.history = {
            "results": {
                "qs"    : np.zeros((self.T, self.K, num_state_categories)), 
                "G"     : np.zeros((self.T, self.K, num_policies)), 
                "q_pi"  : np.zeros((self.T, self.K, num_policies)), 
                "p_u"   : np.zeros((self.T, self.K, num_actions)), 
                "action": np.zeros((self.T, self.K)),
                "o"     : np.zeros((self.T, self.K))
            },
            "ToM": {
                "qs"    : np.zeros((self.T, self.K, num_state_categories)), 
                "G"     : np.zeros((self.T, self.K, num_policies)), 
                "q_pi"  : np.zeros((self.T, self.K, num_policies)),
                "action": np.zeros((self.T, self.K))
            }
        }
            
    def _update_history(self, t: int, k: int, step_results: dict) -> None:
        """ Updates results for time step t and agent k with step results """
        
        # Store agent's main results for the current step
        self.history["results"]["qs"][t, k]   = step_results["qs"][0]
        self.history["results"]["G"][t, k]    = step_results["exp_G"]
        self.history["results"]["q_pi"][t, k] = step_results["exp_q_pi"]
        self.history["results"]["p_u"][t, k]  = step_results["exp_p_u"]
        self.history["results"]["action"][t, k]  = step_results["exp_action"]
        self.history["results"]["o"][t]       = step_results["o"]
        
        # Store agent's theory of mind results for the current step
        tom_results = step_results["tom_results"][0]
        self.history["ToM"]["qs"][t, k]   = tom_results["qs"][0]
        self.history["ToM"]["G"][t, k]    = tom_results["G"]
        self.history["ToM"]["q_pi"][t, k] = tom_results["q_pi"]
        self.history["ToM"]["action"][t]  = tom_results["action"]
        
